{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-58a3eb818962>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look at data folder structure\n",
    "classes_dir = listdir(\"data/Kather_texture_2016_image_tiles_5000\")\n",
    "classes_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(\"data/Kather_texture_2016_image_tiles_5000\"))\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = (imagePath.split(os.path.sep)[-2][1])\n",
    "    # Since we are going to use MobileNetV2 we need to resize the images\n",
    "    # to the expected size by the pre-trained network.\n",
    "    image = load_img(imagePath, target_size=(128, 128)) # Resize image #image = load_img(imagePath, target_size=(28, 28)) # Resize image\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 #normalize\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display resized image\n",
    "plt.imshow(data[0]*255)\n",
    "#plt.imshow(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # perform one-hot encoding on the labels\n",
    "# lb = LabelBinarizer()\n",
    "# labels = lb.fit_transform(labels)\n",
    "# # labels = to_categorical(labels)\n",
    "# print(labels[0], labels[0][0], type(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Split into train, val, test set\n",
    "# x, x_test, y, y_test = train_test_split(data, labels, test_size=0.2,train_size=0.8)\n",
    "# # x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.25,train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "VAE encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space: a 3D space\n",
    "latent_dim = 2\n",
    "encoder_inputs = keras.Input(shape=(128, 128, 3)) # 1 for grayscale\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=1, padding=\"same\")(encoder_inputs) #working with strides and not max pooling\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "# The input image ends up being encoded into these\n",
    "# two parameters (parameters are being predicted by model!)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Sampler(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, z_mean, z_log_var):\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        z_size = tf.shape(z_mean)[1]\n",
    "        # Draw a batch of random normal\n",
    "        # Apply the VAE vectors.\n",
    "        epsilon = tf.random.normal(shape=(batch_size, z_size))\n",
    "        # Apply the VAE vectors sampling formula\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input where we’ll feed z\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# Produce the same number of coefficients that we\n",
    "# had at the level of the Flatten layer in the encoder\n",
    "x = layers.Dense(32 * 32 * 64, activation=\"relu\")(latent_inputs) # adjust if bigger output needed\n",
    "# Revert the Flatten layer of the encoder\n",
    "x = layers.Reshape((32, 32, 64))(x) # equak adjustment here\n",
    "# Revert the Conv2D layers of the encoder\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# The output ends up with shape (28, 28, 3)\n",
    "decoder_outputs = layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sampler = Sampler()\n",
    "        # We use these metrics to keep track of the loss averages\n",
    "        # over each epoch.\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "        name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        # We list the metrics in the metrics\n",
    "        # property to enable the model to reset\n",
    "        # them after each epoch (or between\n",
    "        # multiple calls to fit()/evaluate())\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = self.sampler(z_mean, z_log_var)\n",
    "            reconstruction = decoder(z)\n",
    "            # We sum the reconstruction loss over the spatial\n",
    "            # dimensions (axes 1, 2) and take its mean over the\n",
    "            # batch dimension.\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2))\n",
    "            )\n",
    "            # Add the regularization term (Kullback–Leibler divergence)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss) #complicated part,math in slides\n",
    "        self.kl_loss_tracker.update_state(kl_loss) #complicated part,math in slides\n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train, val, test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2,train_size=0.8)\n",
    "#  Train on all train / test images without labels\n",
    "all_images = np.concatenate([x_train, x_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print('Training set:', all_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Vae\n",
    "vae = VAE(encoder, decoder)\n",
    "# Note that we don’t pass a loss argument in compile(), since the loss\n",
    "# is already part of the train_step().\n",
    "vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n",
    "# Note that we don’t pass targets in fit(), since train_step()\n",
    "# doesn’t expect any\n",
    "history = vae.fit(all_images, epochs=30, batch_size=128)\n",
    "# history = vae.fit(x=x, y= y, epochs=20, batch_size=32, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We’ll display a grid of 30 x 30\n",
    "# digits (900 digits total)\n",
    "# Generating/predicting images based on the latent space\n",
    "n = 128 # 30\n",
    "digit_size = 128\n",
    "figure = np.zeros((digit_size * n, digit_size * n, 3))\n",
    "\n",
    "# Sample points linearly on a 2D grid\n",
    "grid_x = np.linspace(-1, 1, n)\n",
    "grid_y = np.linspace(-1, 1, n)[::-1]\n",
    "\n",
    "# Iterate over grid locations\n",
    "for i, yi in enumerate(grid_y):\n",
    "    for j, xi in enumerate(grid_x):\n",
    "        # For each location, sample a digit and\n",
    "        # add it to our figure\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = vae.decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size, 3)\n",
    "        figure[\n",
    "            i * digit_size : (i + 1) * digit_size ,\n",
    "            j * digit_size : (j + 1) * digit_size ,\n",
    "        ] = digit\n",
    "        plt.imshow(digit*255)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "start_range = digit_size // 2\n",
    "end_range = n * digit_size + start_range\n",
    "pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "sample_range_x = np.round(grid_x, 1)\n",
    "sample_range_y = np.round(grid_y, 1)\n",
    "plt.xticks(pixel_range, sample_range_x)\n",
    "plt.yticks(pixel_range, sample_range_y)\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(figure*255) # plt.imshow(figure, cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "  # ---> 18     x_decoded = vae.decoder.predict(z_sample)\n",
    "  # ValueError: Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
