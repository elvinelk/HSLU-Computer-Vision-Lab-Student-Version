{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['.DS_Store',\n '01_TUMOR',\n '02_STROMA',\n '03_COMPLEX',\n '04_LYMPHO',\n '05_DEBRIS',\n '06_MUCOSA',\n '07_ADIPOSE',\n '08_EMPTY']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at data folder structure\n",
    "classes_dir = listdir(\"data/Kather_texture_2016_image_tiles_5000\")\n",
    "classes_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(\"data/Kather_texture_2016_image_tiles_5000\"))\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = (imagePath.split(os.path.sep)[-2][1])\n",
    "    # Since we are going to use MobileNetV2 we need to resize the images\n",
    "    # to the expected size by the pre-trained network.\n",
    "    image = load_img(imagePath, target_size=(28, 28)) # Resize image\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 #normalize\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x23b426a0b80>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display resized image\n",
    "plt.imshow(data[0]*255)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # perform one-hot encoding on the labels\n",
    "# lb = LabelBinarizer()\n",
    "# labels = lb.fit_transform(labels)\n",
    "# # labels = to_categorical(labels)\n",
    "# print(labels[0], labels[0][0], type(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train, val, test set\n",
    "# x, x_test, y, y_test = train_test_split(data, labels, test_size=0.2,train_size=0.8)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.25,train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "VAE encoder network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space: a 3D space\n",
    "latent_dim = 3\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 3)) # 1 for grayscale\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs) #working with strides and not max pooling\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "# The input image ends up being encoded into these\n",
    "# two parameters (parameters are being predicted by model!)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 28, 28, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 14, 14, 32)   896         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 7, 7, 64)     18496       ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3136)         0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           50192       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69,652\n",
      "Trainable params: 69,652\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Sampler(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, z_mean, z_log_var):\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        z_size = tf.shape(z_mean)[1]\n",
    "        # Draw a batch of random normal\n",
    "        # Apply the VAE vectors.\n",
    "        epsilon = tf.random.normal(shape=(batch_size, z_size))\n",
    "        # Apply the VAE vectors sampling formula\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Input where we’ll feed z\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# Produce the same number of coefficients that we\n",
    "# had at the level of the Flatten layer in the encoder\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# Revert the Flatten layer of the encoder\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "# Revert the Conv2D layers of the encoder\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# The output ends up with shape (28, 28, 3)\n",
    "decoder_outputs = layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3136)              12544     \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_10 (Conv2D  (None, 14, 14, 64)       36928     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_11 (Conv2D  (None, 28, 28, 32)       18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 28, 28, 3)         867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,803\n",
      "Trainable params: 68,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sampler = Sampler()\n",
    "        # We use these metrics to keep track of the loss averages\n",
    "        # over each epoch.\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "        name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        # We list the metrics in the metrics\n",
    "        # property to enable the model to reset\n",
    "        # them after each epoch (or between\n",
    "        # multiple calls to fit()/evaluate())\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = self.sampler(z_mean, z_log_var)\n",
    "            reconstruction = decoder(z)\n",
    "            # We sum the reconstruction loss over the spatial\n",
    "            # dimensions (axes 1, 2) and take its mean over the\n",
    "            # batch dimension.\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2))\n",
    "            )\n",
    "            # Add the regularization term (Kullback–Leibler divergence)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss) #complicated part,math in slides\n",
    "        self.kl_loss_tracker.update_state(kl_loss) #complicated part,math in slides\n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the VAE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Split into train, val, test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2,train_size=0.8)\n",
    "# Train on all train / test images without labels\n",
    "all_images = np.concatenate([x_train, x_test], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "40/40 [==============================] - 10s 241ms/step - total_loss: 222.5094 - reconstruction_loss: 213.4863 - kl_loss: 9.1947\n",
      "Epoch 2/30\n",
      "40/40 [==============================] - 12s 291ms/step - total_loss: 7.5083 - reconstruction_loss: 6.7557 - kl_loss: 0.7592\n",
      "Epoch 3/30\n",
      "40/40 [==============================] - 9s 237ms/step - total_loss: 4.5613 - reconstruction_loss: 4.2472 - kl_loss: 0.3134\n",
      "Epoch 4/30\n",
      "40/40 [==============================] - 10s 245ms/step - total_loss: 1.2682 - reconstruction_loss: 0.0409 - kl_loss: 1.2130\n",
      "Epoch 5/30\n",
      "40/40 [==============================] - 15s 388ms/step - total_loss: -745.7750 - reconstruction_loss: -1279.0637 - kl_loss: 541.0186\n",
      "Epoch 6/30\n",
      "40/40 [==============================] - 14s 350ms/step - total_loss: -64464.6992 - reconstruction_loss: -151207.5938 - kl_loss: 88823.4609\n",
      "Epoch 7/30\n",
      "40/40 [==============================] - 12s 311ms/step - total_loss: -64218.2266 - reconstruction_loss: -64301.9492 - kl_loss: 81.7038\n",
      "Epoch 8/30\n",
      "40/40 [==============================] - 10s 253ms/step - total_loss: 16165864.0000 - reconstruction_loss: -9056991.0000 - kl_loss: 25828198.0000\n",
      "Epoch 9/30\n",
      "40/40 [==============================] - 10s 254ms/step - total_loss: -212540.3281 - reconstruction_loss: -212577.0000 - kl_loss: 36.9714\n",
      "Epoch 10/30\n",
      "40/40 [==============================] - 10s 260ms/step - total_loss: -273508.1250 - reconstruction_loss: -273552.5312 - kl_loss: 44.9816\n",
      "Epoch 11/30\n",
      "40/40 [==============================] - 10s 247ms/step - total_loss: -356215.4688 - reconstruction_loss: -356278.0312 - kl_loss: 62.6431\n",
      "Epoch 12/30\n",
      "40/40 [==============================] - 10s 257ms/step - total_loss: -440545.2500 - reconstruction_loss: -440624.3438 - kl_loss: 78.6667\n",
      "Epoch 13/30\n",
      "40/40 [==============================] - 10s 249ms/step - total_loss: -531223.7500 - reconstruction_loss: -531318.6250 - kl_loss: 94.4517\n",
      "Epoch 14/30\n",
      "40/40 [==============================] - 10s 249ms/step - total_loss: -631888.4375 - reconstruction_loss: -631998.6875 - kl_loss: 110.7811\n",
      "Epoch 15/30\n",
      "40/40 [==============================] - 10s 247ms/step - total_loss: -741872.0625 - reconstruction_loss: -741998.8125 - kl_loss: 128.3222\n",
      "Epoch 16/30\n",
      "40/40 [==============================] - 10s 248ms/step - total_loss: -898440.3750 - reconstruction_loss: -898588.8125 - kl_loss: 147.7812\n",
      "Epoch 17/30\n",
      "40/40 [==============================] - 10s 247ms/step - total_loss: -1077456.2500 - reconstruction_loss: -1077627.2500 - kl_loss: 170.2879\n",
      "Epoch 18/30\n",
      "40/40 [==============================] - 10s 245ms/step - total_loss: -1297620.2500 - reconstruction_loss: -1297816.6250 - kl_loss: 196.7867\n",
      "Epoch 19/30\n",
      "40/40 [==============================] - 11s 283ms/step - total_loss: -1601776.5000 - reconstruction_loss: -1602006.5000 - kl_loss: 229.0699\n",
      "Epoch 20/30\n",
      "40/40 [==============================] - 11s 284ms/step - total_loss: -2042289.6250 - reconstruction_loss: -2042563.0000 - kl_loss: 271.5180\n",
      "Epoch 21/30\n",
      "40/40 [==============================] - 11s 279ms/step - total_loss: -2654614.5000 - reconstruction_loss: -2654954.5000 - kl_loss: 340.9702\n",
      "Epoch 22/30\n",
      "40/40 [==============================] - 11s 264ms/step - total_loss: -3823987.5000 - reconstruction_loss: -3824467.5000 - kl_loss: 476.1549\n",
      "Epoch 23/30\n",
      "40/40 [==============================] - 12s 295ms/step - total_loss: -6087783.5000 - reconstruction_loss: -6088639.0000 - kl_loss: 846.6647\n",
      "Epoch 24/30\n",
      "40/40 [==============================] - 10s 254ms/step - total_loss: -15338834.0000 - reconstruction_loss: -15411646.0000 - kl_loss: 43740.8008\n",
      "Epoch 25/30\n",
      "40/40 [==============================] - 10s 256ms/step - total_loss: -147553696.0000 - reconstruction_loss: -329292640.0000 - kl_loss: 186099024.0000\n",
      "Epoch 26/30\n",
      "40/40 [==============================] - 10s 240ms/step - total_loss: -211059920.0000 - reconstruction_loss: -1080327040.0000 - kl_loss: 890129472.0000\n",
      "Epoch 27/30\n",
      "40/40 [==============================] - 10s 243ms/step - total_loss: -1929791.3750 - reconstruction_loss: -1929799.3750 - kl_loss: 8.2388\n",
      "Epoch 28/30\n",
      "40/40 [==============================] - 10s 242ms/step - total_loss: -2046181.2500 - reconstruction_loss: -2046189.0000 - kl_loss: 8.0809\n",
      "Epoch 29/30\n",
      "40/40 [==============================] - 10s 245ms/step - total_loss: -2680046.5000 - reconstruction_loss: -2680057.7500 - kl_loss: 11.0658\n",
      "Epoch 30/30\n",
      "40/40 [==============================] - 10s 262ms/step - total_loss: -3011025.2500 - reconstruction_loss: -3011037.5000 - kl_loss: 12.2766\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x23b4fa04850>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vae\n",
    "vae = VAE(encoder, decoder)\n",
    "# Note that we don’t pass a loss argument in compile(), since the loss\n",
    "# is already part of the train_step().\n",
    "vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n",
    "# Note that we don’t pass targets in fit(), since train_step()\n",
    "# doesn’t expect any\n",
    "vae.fit(all_images, epochs=30, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 18\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, xi \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(grid_x):\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# For each location, sample a digit and\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m# add it to our figure\u001B[39;00m\n\u001B[0;32m     17\u001B[0m     z_sample \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([[xi, yi]])\n\u001B[1;32m---> 18\u001B[0m     x_decoded \u001B[38;5;241m=\u001B[39m \u001B[43mvae\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz_sample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     digit \u001B[38;5;241m=\u001B[39m x_decoded[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mreshape(digit_size, digit_size)\n\u001B[0;32m     20\u001B[0m     figure[\n\u001B[0;32m     21\u001B[0m         i \u001B[38;5;241m*\u001B[39m digit_size : (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m digit_size,\n\u001B[0;32m     22\u001B[0m         j \u001B[38;5;241m*\u001B[39m digit_size : (j \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m digit_size,\n\u001B[0;32m     23\u001B[0m     ] \u001B[38;5;241m=\u001B[39m digit\n",
      "File \u001B[1;32mc:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev0f61l0n.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\levin\\documents\\hslu\\fs_23\\computer_vision\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n"
     ]
    }
   ],
   "source": [
    "# We’ll display a grid of 30 x 30\n",
    "# digits (900 digits total)\n",
    "# Generating/predicting images based on the latent space\n",
    "n = 30\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "# Sample points linearly on a 2D grid\n",
    "grid_x = np.linspace(-1, 1, n)\n",
    "grid_y = np.linspace(-1, 1, n)[::-1]\n",
    "\n",
    "# Iterate over grid locations\n",
    "for i, yi in enumerate(grid_y):\n",
    "    for j, xi in enumerate(grid_x):\n",
    "        # For each location, sample a digit and\n",
    "        # add it to our figure\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = vae.decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[\n",
    "            i * digit_size : (i + 1) * digit_size,\n",
    "            j * digit_size : (j + 1) * digit_size,\n",
    "        ] = digit\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "start_range = digit_size // 2\n",
    "end_range = n * digit_size + start_range\n",
    "pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "sample_range_x = np.round(grid_x, 1)\n",
    "sample_range_y = np.round(grid_y, 1)\n",
    "plt.xticks(pixel_range, sample_range_x)\n",
    "plt.yticks(pixel_range, sample_range_y)\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(figure, cmap=\"Greys_r\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "  # ---> 18     x_decoded = vae.decoder.predict(z_sample)\n",
    "  # ValueError: Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}